============================================================================
File: scripts/codereview/cmd/compile-context/main.go
Line: 37 to 41
Type: potential_issue

Comment:
Incomplete error handling for os.Stat.

Only os.IsNotExist is checked, but other errors (permission denied, symlink issues, etc.) are silently ignored. The directory might exist but be inaccessible.



Proposed fix

 func run() error {
        // Validate input directory exists
-       if _, err := os.Stat(*inputDir); os.IsNotExist(err) {
-               return fmt.Errorf("input directory does not exist: %s", *inputDir)
+       if _, err := os.Stat(*inputDir); err != nil {
+               if os.IsNotExist(err) {
+                       return fmt.Errorf("input directory does not exist: %s", *inputDir)
+               }
+               return fmt.Errorf("cannot access input directory %s: %w", *inputDir, err)
        }

Prompt for AI Agent:
In @scripts/codereview/cmd/compile-context/main.go around lines 37 - 41, The os.Stat call in run() only checks os.IsNotExist(err) and ignores other errors (e.g., permission or symlink issues); update the logic to handle all non-nil errors by returning a wrapped error with context (including *inputDir and the original err) when err != nil, and only proceed if err == nil; reference run(), the os.Stat call, and the inputDir variable when making the change.



============================================================================
File: scripts/codereview/internal/context/reviewer_mappings_test.go
Line: 70 to 84
Type: nitpick

Comment:
Add test for unknown severity threshold behavior.

FilterFindingsBySeverity returns all findings when given an unknown severity threshold, but this behavior isn't tested. Based on learnings, edge cases should be covered.



Proposed additional test

func TestFilterFindingsBySeverity_UnknownThreshold(t *testing.T) {
        findings := []Finding{
                {Severity: "critical", Message: "crash"},
                {Severity: "info", Message: "hint"},
        }

        // Unknown severity threshold should return all findings
        result := FilterFindingsBySeverity(findings, "unknown")
        if len(result) != 2 {
                t.Errorf("FilterFindingsBySeverity(unknown) returned %d, want 2 (all findings)", len(result))
        }
}

Prompt for AI Agent:
In @scripts/codereview/internal/context/reviewer_mappings_test.go around lines 70 - 84, Add a unit test that verifies FilterFindingsBySeverity returns all findings when passed an unrecognized severity string: create TestFilterFindingsBySeverity_UnknownThreshold that constructs a small slice of Finding (e.g., "critical" and "info"), calls FilterFindingsBySeverity with "unknown", and asserts the returned slice length equals the original length (and/or contents), so the unknown-threshold behavior is covered.



============================================================================
File: scripts/codereview/internal/context/reviewer_mappings_test.go
Line: 28 to 49
Type: nitpick

Comment:
Add test case for unknown reviewer.

The test validates known reviewers but doesn't verify behavior for an unknown reviewer, which should return an empty slice per the implementation.



Proposed additional test case

        tests := []struct {
                reviewer string
                wantLen  int
        }{
                {"code-reviewer", 2},           // static-analysis, semantic-diff
                {"security-reviewer", 2},       // static-analysis (security), security-summary
                {"business-logic-reviewer", 2}, // semantic-diff, impact-summary
                {"test-reviewer", 1},           // impact-summary (test coverage)
                {"nil-safety-reviewer", 1},     // data-flow (nil_sources)
+               {"unknown-reviewer", 0},        // unknown reviewer returns empty slice
        }

Prompt for AI Agent:
In @scripts/codereview/internal/context/reviewer_mappings_test.go around lines 28 - 49, The test TestGetReviewerDataSources misses a case for unknown reviewers; add a test entry using GetReviewerDataSources with a reviewer string like "unknown-reviewer" and assert it returns an empty slice (length 0). Update the tests slice in TestGetReviewerDataSources to include {"unknown-reviewer", 0} (or an equivalent t.Run that calls GetReviewerDataSources("unknown-reviewer") and checks len(sources)==0) so the behavior for unknown reviewers is validated.



============================================================================
File: scripts/codereview/internal/context/reviewer_mappings.go
Line: 103 to 109
Type: potential_issue

Comment:
Findings with unknown severity are silently dropped.

When a finding has a severity not in severityOrder (e.g., "medium", "error", or typos), it's silently excluded from results. This may cause unexpected data loss. Consider logging or including such findings with a default behavior.



Proposed fix to include findings with unknown severity

        var filtered []Finding
        for _, f := range findings {
-               if level, ok := severityOrder[f.Severity]; ok && level >= minLevel {
+               level, ok := severityOrder[f.Severity]
+               if !ok {
+                       // Include findings with unknown severity to avoid silent data loss
+                       filtered = append(filtered, f)
+                       continue
+               }
+               if level >= minLevel {
                        filtered = append(filtered, f)
                }
        }

Prompt for AI Agent:
In @scripts/codereview/internal/context/reviewer_mappings.go around lines 103 - 109, The loop that builds filtered currently drops any Finding whose f.Severity key is missing from severityOrder; update the logic in the block that iterates over findings (variables: findings, filtered, f) to handle unknown severities instead of silently excluding them: when severityOrder[f.Severity] lookup returns ok==false, either assign a default level (e.g., defaultLevel := 0 or whatever represents lowest severity) and proceed to compare against minLevel, or append the Finding unconditionally and emit a warning via your logger about the unknown severity; ensure you reference severityOrder, minLevel and Finding so the change is applied in the same loop and preserve existing behavior for known severities.



============================================================================
File: scripts/codereview/internal/context/reviewer_mappings.go
Line: 64 to 73
Type: nitpick

Comment:
Potential drift between GetReviewerNames() and reviewerDataSources map.

The reviewer names are hardcoded in GetReviewerNames() while reviewerDataSources is a separate map. If one is updated without the other, they will diverge. Consider deriving the list from the map keys to ensure consistency.



Alternative approach using map keys

+// reviewerOrder defines the canonical order of reviewers.
+var reviewerOrder = []string{
+       "code-reviewer",
+       "security-reviewer",
+       "business-logic-reviewer",
+       "test-reviewer",
+       "nil-safety-reviewer",
+}
+
 // GetReviewerNames returns the list of all reviewer names in order.
 func GetReviewerNames() []string {
-       return []string{
-               "code-reviewer",
-               "security-reviewer",
-               "business-logic-reviewer",
-               "test-reviewer",
-               "nil-safety-reviewer",
-       }
+       return reviewerOrder
 }


Then validate in an init() function or test that all reviewerOrder entries exist in reviewerDataSources.

Prompt for AI Agent:
In @scripts/codereview/internal/context/reviewer_mappings.go around lines 64 - 73, GetReviewerNames currently returns a hardcoded slice which can drift from the reviewerDataSources map; change GetReviewerNames to derive its ordered list from the authoritative source (either iterate reviewerOrder to build the slice and validate each key exists in reviewerDataSources, or derive keys from reviewerDataSources then sort/arrange according to reviewerOrder) and add a sanity check (init() or a test) that every entry in reviewerOrder exists in reviewerDataSources so the two stay consistent; reference GetReviewerNames, reviewerDataSources and reviewerOrder when implementing the change and the validation.



============================================================================
File: scripts/codereview/install.sh
Line: 284 to 292
Type: nitpick

Comment:
Duplicate BINARIES array declaration.

The BINARIES array is defined identically in both build_binaries() (lines 221-229) and verify_installation() (lines 284-292). Consider extracting this to a global constant at the top of the script to maintain DRY principle and ensure consistency if binaries are added/removed.




Proposed refactor

Add near the top of the script (after line 7):
# List of all codereview binaries
BINARIES=(
    "scope-detector"
    "static-analysis"
    "ast-extractor"
    "call-graph"
    "data-flow"
    "compile-context"
    "run-all"
)


Then remove the local declarations in build_binaries() and verify_installation().

Prompt for AI Agent:
In @scripts/codereview/install.sh around lines 284 - 292, The BINARIES array is duplicated in build_binaries() and verify_installation(); extract it to a single global constant so additions stay in sync. Add a single BINARIES declaration near the top of the script (after the shebang/initial comments) containing the listed binaries, then remove the local BINARIES definitions from build_binaries() and verify_installation(), updating those functions to reference the global BINARIES variable instead.



============================================================================
File: scripts/codereview/internal/context/compiler.go
Line: 234 to 252
Type: nitpick

Comment:
Consider using a map or registry pattern for reviewer data builders.

The switch statement works but adding new reviewers requires modifying this function. A registry pattern would make the code more extensible.




Alternative approach using a map

type reviewerDataBuilder func(Compiler, TemplateData, *PhaseOutputs)

var reviewerDataBuilders = map[string]reviewerDataBuilder{
    "code-reviewer":           (*Compiler).buildCodeReviewerData,
    "security-reviewer":       (*Compiler).buildSecurityReviewerData,
    "business-logic-reviewer": (*Compiler).buildBusinessLogicReviewerData,
    "test-reviewer":           (*Compiler).buildTestReviewerData,
    "nil-safety-reviewer":     (*Compiler).buildNilSafetyReviewerData,
}

func (c Compiler) buildTemplateData(reviewer string, outputs PhaseOutputs) *TemplateData {
    data := &TemplateData{}
    if builder, ok := reviewerDataBuilders[reviewer]; ok {
        builder(c, data, outputs)
    }
    return data
}

Prompt for AI Agent:
In @scripts/codereview/internal/context/compiler.go around lines 234 - 252, The switch in buildTemplateData makes adding reviewers require editing the function; replace it with a registry map of builders keyed by reviewer name (define a reviewerDataBuilder type and a reviewerDataBuilders map mapping strings to methods like (Compiler).buildCodeReviewerData, (Compiler).buildSecurityReviewerData, etc.), then in Compiler.buildTemplateData look up reviewer in reviewerDataBuilders and, if found, call builder(c, data, outputs); this keeps buildTemplateData unchanged when adding new reviewers and uses the existing methods buildCodeReviewerData, buildSecurityReviewerData, buildBusinessLogicReviewerData, buildTestReviewerData, and buildNilSafetyReviewerData.



============================================================================
File: scripts/codereview/internal/context/reviewer_mappings.go
Line: 32 to 37
Type: potential_issue

Comment:
Missing "medium" severity level.

The severityOrder map lacks a "medium" severity level, which is referenced elsewhere in this codebase (e.g., NilSource.Risk uses "medium" in tests at line 91 of the test file). Findings with "medium" severity will be silently dropped by FilterFindingsBySeverity.



Proposed fix

 var severityOrder = map[string]int{
        "critical": 4,
        "high":     3,
+       "medium":   2,
        "warning":  2,
        "info":     1,
 }

Prompt for AI Agent:
In @scripts/codereview/internal/context/reviewer_mappings.go around lines 32 - 37, Add the missing "medium" severity to the severityOrder map and adjust the numeric ordering so severities are strictly ordered (e.g., "critical":4, "high":3, "medium":2, "warning":1, "info":0) to ensure FilterFindingsBySeverity and tests (e.g., NilSource.Risk) correctly include "medium" findings; update the severityOrder variable accordingly.



============================================================================
File: scripts/codereview/internal/context/reviewer_mappings_test.go
Line: 7 to 26
Type: nitpick

Comment:
Consider using reflect.DeepEqual for slice comparison.

The manual element-by-element comparison can be simplified.



Simplified comparison

+import (
+       "reflect"
+       "testing"
+)
+
 func TestReviewerNames(t *testing.T) {
        expected := []string{
                "code-reviewer",
                "security-reviewer",
                "business-logic-reviewer",
                "test-reviewer",
                "nil-safety-reviewer",
        }

        names := GetReviewerNames()
-       if len(names) != len(expected) {
-               t.Errorf("GetReviewerNames() returned %d names, want %d", len(names), len(expected))
-       }
-
-       for i, name := range expected {
-               if names[i] != name {
-                       t.Errorf("GetReviewerNames()[%d] = %q, want %q", i, names[i], name)
-               }
+       if !reflect.DeepEqual(names, expected) {
+               t.Errorf("GetReviewerNames() = %v, want %v", names, expected)
        }
 }

Prompt for AI Agent:
In @scripts/codereview/internal/context/reviewer_mappings_test.go around lines 7 - 26, The test TestReviewerNames currently compares slices element-by-element; replace that logic with a single reflect.DeepEqual check: call GetReviewerNames(), then use reflect.DeepEqual(expected, names) to assert equality and call t.Errorf (or t.Fatalf) with a clear message showing expected and actual when the comparison fails; ensure you import the reflect package and remove the length and loop checks.



============================================================================
File: scripts/codereview/internal/context/types.go
Line: 112 to 115
Type: nitpick

Comment:
Type naming collision: TypeInfo is overloaded.

TypeInfo at line 113 holds return type information (single Type field), but there's also TypeChanges and TypeDiff for struct/type changes. The name TypeInfo is ambiguous. Consider renaming to ReturnTypeInfo or similar for clarity.

Prompt for AI Agent:
In @scripts/codereview/internal/context/types.go around lines 112 - 115, The TypeInfo struct is ambiguously named; rename the struct TypeInfo to ReturnTypeInfo (keeping the Type string json:"type" field) and update all references/usages (variables, function parameters/returns, imports, tests) to use ReturnTypeInfo instead of TypeInfo so it no longer collides conceptually with TypeChanges/TypeDiff; rebuild and run tests to ensure all references (including any JSON marshaling/unmarshaling code) are updated to the new identifier.



============================================================================
File: scripts/codereview/install.sh
Line: 325 to 330
Type: nitpick

Comment:
Consolidate file redirects per static analysis hint.

The static analysis tool suggests using grouped redirects for cleaner file writing.




Proposed fix

-    echo "# Checksums for codereview tool dependencies" > "$CHECKSUMS_FILE"
-    echo "# Generated on $(date -u +"%Y-%m-%dT%H:%M:%SZ")" >> "$CHECKSUMS_FILE"
-    echo "# Format: tool_name: sha256_checksum" >> "$CHECKSUMS_FILE"
-    echo "#" >> "$CHECKSUMS_FILE"
-    echo "# To update: run './install.sh generate-checksums' after verifying tool integrity" >> "$CHECKSUMS_FILE"
-    echo "" >> "$CHECKSUMS_FILE"
+    {
+        echo "# Checksums for codereview tool dependencies"
+        echo "# Generated on $(date -u +"%Y-%m-%dT%H:%M:%SZ")"
+        echo "# Format: tool_name: sha256_checksum"
+        echo "#"
+        echo "# To update: run './install.sh generate-checksums' after verifying tool integrity"
+        echo ""
+    } > "$CHECKSUMS_FILE"

Prompt for AI Agent:
In @scripts/codereview/install.sh around lines 325 - 330, The script writes multiple echo lines to the same target ($CHECKSUMS_FILE) causing repeated redirects; consolidate these into a single grouped redirect (here-doc or single cat  "$CHECKSUMS_FILE") to write all header lines at once in install.sh, referencing the CHECKSUMS_FILE variable and the current date substitution inside the here-doc or by building the content into a variable first then redirecting once. Ensure the grouped block preserves the exact lines (including the blank line) and uses proper quoting to avoid unintended variable expansion.



============================================================================
File: scripts/codereview/cmd/ast-extractor/main.go
Line: 88 to 103
Type: nitpick

Comment:
Duplicate helper - same as call-graph/main.go.

As noted in the call-graph review, this should be extracted to a shared internal package.

Prompt for AI Agent:
In @scripts/codereview/cmd/ast-extractor/main.go around lines 88 - 103, The readJSONFileWithLimit function and maxJSONFileSize constant are duplicated (also defined in call-graph/main.go); extract them into a shared internal package (e.g., internal/io or internal/fileutil) and replace the local definitions with imports and calls to the shared helper; move the implementation (including const maxJSONFileSize and func readJSONFileWithLimit) to a new package, update both scripts/codereview/cmd/ast-extractor/main.go and call-graph/main.go to import that package and call fileutil.ReadJSONFileWithLimit (or similar exported names), and remove the duplicate local definitions.



============================================================================
File: scripts/codereview/install.sh
Line: 214 to 240
Type: potential_issue

Comment:
Missing cd restoration after build_binaries.

Similar to install_ts_tools, this function changes directory at line 218 but doesn't restore it. While set -e would exit on failure, successful execution leaves the working directory changed.




Proposed fix using subshell

 build_binaries() {
     log_info "Building codereview binaries..."

     mkdir -p "${BIN_DIR}"
-    cd "${SCRIPT_DIR}"
+    (
+        cd "${SCRIPT_DIR}"

-    # List of all binaries to build
-    BINARIES=(
-        ...
-    )
+        # List of all binaries to build
+        local BINARIES=(
+            ...
+        )

-    for binary in "${BINARIES[@]}"; do
-        log_info "Building ${binary}..."
-        go build -o "${BIN_DIR}/${binary}" "./cmd/${binary}"
-    done
+        for binary in "${BINARIES[@]}"; do
+            log_info "Building ${binary}..."
+            go build -o "${BIN_DIR}/${binary}" "./cmd/${binary}"
+        done

-    # Make all binaries executable
-    chmod +x "${BIN_DIR}"/*
+        # Make all binaries executable
+        chmod +x "${BIN_DIR}"/*

-    log_info "All binaries built successfully in ${BIN_DIR}/"
+        log_info "All binaries built successfully in ${BIN_DIR}/"
+    )
 }

Prompt for AI Agent:
In @scripts/codereview/install.sh around lines 214 - 240, The build_binaries function changes directory to "${SCRIPT_DIR}" and never restores the original working directory, leaving callers in a different CWD after a successful run; modify build_binaries (the function that uses BINARIES, BIN_DIR and SCRIPT_DIR) to either perform the build steps inside a subshell (e.g., wrap the cd and build loop in parentheses) or save the current directory and restore it (store PWD and cd back or use pushd/popd) so the original working directory is restored on function exit.



============================================================================
File: scripts/codereview/internal/context/types.go
Line: 339 to 344
Type: nitpick

Comment:
ReviewerContext fields lack JSON tags.

Unlike other types, ReviewerContext has no JSON tags. If this type is intended for JSON serialization, add tags. If it's only used in-memory, consider documenting this distinction.

Based on learnings about maintaining consistent field order and documentation across related structs.



‚ôªÔ∏è Add JSON tags if serialization is needed

 type ReviewerContext struct {
-       ReviewerName string
-       Title        string
-       Content      string
+       ReviewerName string json:"reviewer_name"
+       Title        string json:"title"
+       Content      string json:"content"
 }

Prompt for AI Agent:
In @scripts/codereview/internal/context/types.go around lines 339 - 344, ReviewerContext is missing JSON tags on its exported fields (ReviewerName, Title, Content); if this struct is serialized, add consistent json tags (e.g., json:"reviewer_name" / json:"title" / json:"content") to the ReviewerContext type declaration to match other structs and maintain stable field order; if it is only used in-memory, add a brief comment above the ReviewerContext definition stating it is not serialized to make the intent explicit.



============================================================================
File: scripts/codereview/install.sh
Line: 169 to 212
Type: nitpick

Comment:
Consider using --user flag or virtual environment for Python tools.

Installing Python tools globally with pip install can pollute the system environment or fail without root permissions. Consider using --user flag for user-local installation, or recommend using a virtual environment.




Proposed fix to use --user flag

     # ruff - Fast Python linter (pinned version)
     if command -v ruff &> /dev/null; then
         log_info "ruff already installed: $(ruff --version 2>&1)"
     else
-        install_tool "ruff" $PIP_CMD install ruff==0.8.4 || return 1
+        install_tool "ruff" $PIP_CMD install --user ruff==0.8.4 || return 1
     fi


Apply similarly for mypy, pylint, and bandit installations.

Prompt for AI Agent:
In @scripts/codereview/install.sh around lines 169 - 212, The install_py_tools function installs Python packages globally via install_tool using the PIP_CMD variable; change the pip install invocations to use user-local installs or respect virtual environments by adding the --user flag when not inside a venv, or detect VIRTUAL_ENV and skip --user when inside a virtual environment. Update each install_tool call for ruff, mypy, pylint, and bandit to include the --user flag (or conditionally add it based on VIRTUAL_ENV), and ensure the logic that sets PIP_CMD remains unchanged so installs still use pip3 when available. Ensure install_tool still propagates errors as before.



============================================================================
File: scripts/codereview/install.sh
Line: 127 to 167
Type: potential_issue

Comment:
Missing cd restoration after TypeScript installation.

The function changes directory to SCRIPT_DIR at line 136 but never restores the original directory. If another function relies on the current directory, this could cause unexpected behavior. Consider using a subshell or saving/restoring the directory.




Proposed fix using subshell

 install_ts_tools() {
     log_info "Installing TypeScript analysis tools (local to project)..."

     # Check if npm is available
     if ! command -v npm &> /dev/null; then
         log_error "npm is not installed. Please install Node.js first."
         exit 1
     fi

-    cd "${SCRIPT_DIR}"
+    (
+        cd "${SCRIPT_DIR}"

-    # Initialize package.json if it doesn't exist
-    if [[ ! -f "package.json" ]]; then
-        log_info "Initializing package.json..."
-        npm init -y > /dev/null 2>&1
-    fi
+        # Initialize package.json if it doesn't exist
+        if [[ ! -f "package.json" ]]; then
+            log_info "Initializing package.json..."
+            npm init -y > /dev/null 2>&1
+        fi

-    # typescript - TypeScript compiler (pinned version, local install)
-    ...
+        # typescript - TypeScript compiler (pinned version, local install)
+        ...
+    )

Prompt for AI Agent:
In @scripts/codereview/install.sh around lines 127 - 167, The install_ts_tools function changes directory to SCRIPT_DIR with cd "${SCRIPT_DIR}" without restoring the original working directory; wrap the install steps in a subshell or save/restore PWD (e.g., pushd/popd or capture OLD_PWD and cd back) so the function exits leaving the caller's cwd unchanged, and ensure any early returns/errors still restore the directory before returning. Reference: install_ts_tools, the cd "${SCRIPT_DIR}" line and the subsequent npm/install_tool calls.



============================================================================
File: scripts/codereview/cmd/call-graph/main.go
Line: 59 to 74
Type: nitpick

Comment:
Code duplication: readJSONFileWithLimit and maxJSONFileSize are duplicated across CLI tools.

This helper and constant are duplicated in ast-extractor/main.go. Consider extracting to a shared internal package (e.g., internal/fileutil) to maintain DRY principles and ensure consistent behavior across tools.



‚ôªÔ∏è Suggested refactor

Create a shared utility:

// internal/fileutil/json.go
package fileutil

const MaxJSONFileSize = 50  1024  1024

func ReadJSONFileWithLimit(path string) ([]byte, error) {
    // ... implementation
}


Then import in both CLI tools.

Prompt for AI Agent:
In @scripts/codereview/cmd/call-graph/main.go around lines 59 - 74, Duplicate constants and helper readJSONFileWithLimit/maxJSONFileSize should be moved to a shared internal package: create internal/fileutil with exported symbols MaxJSONFileSize and ReadJSONFileWithLimit (mirroring current behavior and error messages), update callers in call-graph's main.go and ast-extractor to import fileutil and call fileutil.ReadJSONFileWithLimit, and remove the local const and function to avoid duplication and ensure consistent behavior across tools.



============================================================================
File: scripts/codereview/internal/context/templates.go
Line: 369 to 385
Type: nitpick

Comment:
GetTemplateForReviewer returns empty string for unknown reviewers.

Callers must handle the empty string case. Consider returning an error or providing a fallback template. Based on learnings, this aligns with the preference for typed accessor functions.



‚ôªÔ∏è Alternative: Return error for unknown reviewer

-func GetTemplateForReviewer(reviewer string) string {
+func GetTemplateForReviewer(reviewer string) (string, error) {
        switch reviewer {
        case "code-reviewer":
-               return codeReviewerTemplate
+               return codeReviewerTemplate, nil
        // ... other cases ...
        default:
-               return ""
+               return "", fmt.Errorf("unknown reviewer: %s", reviewer)
        }
 }

Prompt for AI Agent:
In @scripts/codereview/internal/context/templates.go around lines 369 - 385, GetTemplateForReviewer currently returns an empty string for unknown reviewer keys which forces callers to handle the ambiguous empty value; change the API to either return a (string, error) or a consistent fallback template. Specifically, update GetTemplateForReviewer to return (string, error) and in the switch cases return the template and nil, while the default should return "", fmt.Errorf("unknown reviewer: %s", reviewer); then update all callers to handle the error instead of relying on an empty string (alternatively, if you prefer a non-error path, return a documented defaultTemplate from the default branch and update callers to expect the fallback).



============================================================================
File: scripts/codereview/internal/context/compiler.go
Line: 58 to 70
Type: potential_issue

Comment:
readJSONFileWithLimit is defined but never used.

The function readJSONFileWithLimit is defined to prevent resource exhaustion but readPhaseOutputs uses os.ReadFile directly without size checking. Either use this function in readPhaseOutputs or remove it if unnecessary.




Proposed fix - use the limit function in readPhaseOutputs

 // Read scope.json (Phase 0)
 scopePath := filepath.Join(c.inputDir, "scope.json")
-if data, err := os.ReadFile(scopePath); err == nil {
+if data, err := readJSONFileWithLimit(scopePath); err == nil {
     var scope ScopeData


Apply similarly for all os.ReadFile calls in readPhaseOutputs (lines 134, 145, 158, 174, 190).

Prompt for AI Agent:
In @scripts/codereview/internal/context/compiler.go around lines 58 - 70, The helper readJSONFileWithLimit is implemented but not used; update readPhaseOutputs to replace direct os.ReadFile calls with readJSONFileWithLimit so JSON files are size-checked before reading (use readJSONFileWithLimit(path) instead of os.ReadFile(path)), handle the returned error similarly to existing error handling, and ensure the same replacement is applied for each os.ReadFile invocation inside readPhaseOutputs so maxJSONFileSize is enforced consistently.



============================================================================
File: scripts/codereview/cmd/ast-extractor/main.go
Line: 118 to 123
Type: potential_issue

Comment:
Validation only runs when -scripts is explicitly provided.

When scriptsDir is derived from the executable path (lines 110-115), it bypasses validation. Consider always validating scriptsPath before use, regardless of its source.



üêõ Suggested fix

-       // Validate scripts directory before use (only if explicitly provided)
-       if *scriptDir != "" {
-               if err := validateScriptsDir(scriptsPath); err != nil {
-                       return fmt.Errorf("scripts directory validation failed: %w", err)
-               }
+       // Validate scripts directory before use
+       if err := validateScriptsDir(scriptsPath); err != nil {
+               return fmt.Errorf("scripts directory validation failed: %w", err)
        }

Prompt for AI Agent:
In @scripts/codereview/cmd/ast-extractor/main.go around lines 118 - 123, The code only calls validateScriptsDir(scriptsPath) when *scriptDir != "" so scriptsPath derived from the executable path can bypass validation; always validate scriptsPath after it is determined (i.e., after the logic that sets scriptsPath from the exec path or flag) by calling validateScriptsDir(scriptsPath) unconditionally and returning a wrapped error (fmt.Errorf("scripts directory validation failed: %w", err)) on failure; update the block around scriptDir / scriptsPath assignment to remove the conditional and perform the single validateScriptsDir call so all code paths are validated.



============================================================================
File: scripts/codereview/internal/context/compiler.go
Line: 128 to 203
Type: potential_issue

Comment:
Silently ignoring JSON parse errors for AST, CallGraph, and DataFlow files.

While scope.json and static-analysis.json parse errors are appended to outputs.Errors, parse errors for language-specific files (AST, CallGraph, DataFlow) in lines 156-200 are silently ignored. This inconsistency could make debugging difficult when malformed files exist.




Proposed fix for one example (apply to all)

 for _, lang := range []string{"go", "ts", "py"} {
     astPath := filepath.Join(c.inputDir, fmt.Sprintf("%s-ast.json", lang))
     if data, err := os.ReadFile(astPath); err == nil {
         var ast ASTData
         if err := json.Unmarshal(data, &ast); err == nil {
             outputs.ASTByLanguage[lang] = &ast
             // Keep backward compatibility: first language found becomes primary
             if outputs.AST == nil {
                 outputs.AST = &ast
             }
+        } else {
+            outputs.Errors = append(outputs.Errors, fmt.Sprintf("%s-ast.json parse error: %v", lang, err))
         }
     }
 }

Prompt for AI Agent:
In @scripts/codereview/internal/context/compiler.go around lines 128 - 203, The readPhaseOutputs method (Compiler.readPhaseOutputs) currently appends parse errors for scope.json and static-analysis.json but silently ignores json.Unmarshal errors for per-language AST, CallGraph, and DataFlow files (outputs.ASTByLanguage, outputs.CallGraphByLanguage, outputs.DataFlowByLanguage); update each language loop so that when json.Unmarshal returns an error you append a descriptive error to outputs.Errors (e.g., include the filename like fmt.Sprintf("%s-ast.json parse error: %v", lang, err)) and similarly for "%s-calls.json" and "%s-flow.json", keeping the existing behavior for successful parses and for missing files.



============================================================================
File: scripts/codereview/cmd/scope-detector/main.go
Line: 40 to 46
Type: nitpick

Comment:
Flag binding pattern differs from other CLI tools.

This file uses two consecutive flag.BoolVar calls, while call-graph/main.go and ast-extractor/main.go use flag.Bool with an init() function. Both work correctly, but consider aligning the pattern for consistency across the codebase.

The current approach is actually cleaner since it doesn't require init().

Prompt for AI Agent:
In @scripts/codereview/cmd/scope-detector/main.go around lines 40 - 46, The flag registration here uses flag.BoolVar for cfg.verbose while other CLIs (call-graph/main.go, ast-extractor/main.go) use flag.Bool in an init() function; to align patterns, move the verbose flag definitions into an init() and use flag.Bool to create package-level variables (or keep cfg fields but assign them from the flag.Bool return), replacing the flag.BoolVar calls for "v" and "verbose" with the flag.Bool pattern and ensure cfg.verbose is set from those variables in main() or init().



============================================================================
File: scripts/codereview/internal/context/compiler.go
Line: 25 to 56
Type: potential_issue

Comment:
Path validation is overly restrictive - rejects legitimate paths with ...

The check strings.Contains(path, "..") will reject legitimate paths like /home/user/..hidden (directories starting with ..) or even resolved absolute paths that happen to contain .. in directory names. A more robust approach would be to resolve the path first and then verify it doesn't escape a safe boundary.




Proposed fix

 func validatePath(path string, mustExist bool) error {
-       // Prevent path traversal sequences
-       if strings.Contains(path, "..") {
-               return fmt.Errorf("path cannot contain path traversal sequences '..': %s", path)
-       }
-
        // Get absolute path
        absPath, err := filepath.Abs(path)
        if err != nil {
                return fmt.Errorf("invalid path: %w", err)
        }

+       // Clean the path and check for traversal attempts
+       cleanPath := filepath.Clean(absPath)
+       if cleanPath != absPath {
+               // Path contained .. sequences that changed the result
+               return fmt.Errorf("path contains traversal sequences: %s", path)
+       }
+
        if mustExist {


This approach resolves the path first and only rejects if cleaning it changes the result, which indicates actual traversal sequences.

Prompt for AI Agent:
In @scripts/codereview/internal/context/compiler.go around lines 25 - 56, The initial naive check using strings.Contains(path, "..") is too strict; instead resolve and clean the path first and only reject if cleaning changes the input (indicating real traversal). In validatePath, remove the early strings.Contains check and replace it with: compute cleaned := filepath.Clean(path) and absPath := filepath.Abs(cleaned) (handle abs error), then if cleaned != path && strings.Contains(path, "..") return an error about path traversal; continue using absPath for the subsequent os.Stat and directory checks. This preserves legitimate names containing ".." while blocking actual traversal.



============================================================================
File: scripts/codereview/cmd/ast-extractor/main.go
Line: 57 to 86
Type: potential_issue

Comment:
Path traversal check is incomplete.

The strings.Contains(scriptsDir, "..") check can be bypassed through:
1. URL-encoded sequences (though less relevant for file paths)
2. Symlink chains that resolve outside the intended directory
3. The check doesn't normalize the path before validation

A more robust approach uses filepath.Clean and verifies the result stays within an expected base directory.



üîí Suggested improvement

 func validateScriptsDir(scriptsDir string) error {
-       // Prevent path traversal sequences
-       if strings.Contains(scriptsDir, "..") {
-               return fmt.Errorf("scripts directory cannot contain path traversal sequences '..'")
-       }
-
        // Get absolute path
        absPath, err := filepath.Abs(scriptsDir)
        if err != nil {
                return fmt.Errorf("invalid scripts directory path: %w", err)
        }
+
+       // Clean and verify no traversal occurred
+       cleanPath := filepath.Clean(absPath)
+       if cleanPath != absPath {
+               return fmt.Errorf("scripts directory path contains invalid sequences")
+       }

        // Verify directory exists
        info, err := os.Stat(absPath)

Prompt for AI Agent:
In @scripts/codereview/cmd/ast-extractor/main.go around lines 57 - 86, The path-traversal check in validateScriptsDir is insufficient because strings.Contains(scriptsDir, "..") can be bypassed; instead normalize and resolve the path and compare it against an allowed base: call filepath.Clean on scriptsDir, resolve to an absolute path with filepath.Abs, resolve symlinks with filepath.EvalSymlinks, then resolve the absolute/evaluated base directory the scripts are allowed to live in and use filepath.Rel (or prefix comparison of the evaluated paths) to ensure the target path remains inside that base; also keep the existing os.Stat and IsDir checks and return clear errors if the path is outside the base or cannot be resolved.



============================================================================
File: scripts/codereview/internal/context/compiler_test.go
Line: 57 to 173
Type: nitpick

Comment:
Consider consolidating reviewer-specific tests into a table-driven pattern.

These five tests share identical setup logic. A table-driven approach would reduce duplication while maintaining readability. Based on learnings, table-driven tests are preferred.



‚ôªÔ∏è Suggested refactor to table-driven tests

func TestCompiler_ReviewerContexts(t *testing.T) {
        tests := []struct {
                name           string
                reviewer       string
                expectedTitle  string
                expectedSections []string
        }{
                {
                        name:          "code reviewer has quality sections",
                        reviewer:      "code-reviewer",
                        expectedTitle: "Code Quality",
                        expectedSections: []string{"Static Analysis Findings"},
                },
                {
                        name:          "security reviewer has security sections",
                        reviewer:      "security-reviewer",
                        expectedTitle: "Security",
                        expectedSections: nil,
                },
                // ... other reviewers
        }

        for _, tt := range tests {
                t.Run(tt.name, func(t *testing.T) {
                        inputDir := t.TempDir()
                        outputDir := t.TempDir()
                        createSamplePhaseOutputs(t, inputDir)

                        compiler := NewCompiler(inputDir, outputDir)
                        if err := compiler.Compile(); err != nil {
                                t.Fatalf("Compile() error = %v", err)
                        }

                        content, err := os.ReadFile(filepath.Join(outputDir, "context-"+tt.reviewer+".md"))
                        if err != nil {
                                t.Fatalf("Failed to read %s context: %v", tt.reviewer, err)
                        }

                        contentStr := string(content)
                        if !strings.Contains(contentStr, tt.expectedTitle) {
                                t.Errorf("Missing %q title", tt.expectedTitle)
                        }
                        for _, section := range tt.expectedSections {
                                if !strings.Contains(contentStr, section) {
                                        t.Errorf("Missing section: %s", section)
                                }
                        }
                })
        }
}

Prompt for AI Agent:
In @scripts/codereview/internal/context/compiler_test.go around lines 57 - 173, Multiple tests (TestCompiler_CodeReviewerContext, TestCompiler_SecurityReviewerContext, TestCompiler_BusinessLogicReviewerContext, TestCompiler_TestReviewerContext, TestCompiler_NilSafetyReviewerContext) duplicate setup and assertions; convert them into a single table-driven test. Create a new test (e.g., TestCompiler_ReviewerContexts) that defines a slice of test cases with fields reviewer, expectedTitle, and expectedSections, loop over cases with t.Run, reuse createSamplePhaseOutputs, NewCompiler and Compile, read filepath.Join(outputDir, "context-"+tt.reviewer+".md"), assert that content contains tt.expectedTitle and each tt.expectedSections entry; remove the five redundant tests after ensuring coverage is preserved.



============================================================================
File: scripts/codereview/Makefile
Line: 15 to 58
Type: nitpick

Comment:
Consider a pattern rule to reduce repetition (optional).

The repeated pattern (echo, mkdir, go build) across all phase targets could be consolidated using a pattern rule or define macro. However, the current explicit approach is readable and acceptable for this number of targets.



‚ôªÔ∏è Optional: Pattern rule approach

# Define all phase binaries and their source directories
PHASE_BINARIES := scope-detector static-analysis ast-extractor call-graph data-flow compile-context run-all

# Pattern rule for building phase binaries
$(PHASE_BINARIES):
        @echo "Building $@..."
        @mkdir -p $(BIN_DIR)
        @go build -o $(BIN_DIR)/$@ ./cmd/$@


This assumes each binary has a corresponding cmd/ directory, which appears to be the case.

Prompt for AI Agent:
In @scripts/codereview/Makefile around lines 15 - 58, The Makefile repeats the same build steps for targets like scope-detector, static-analysis, ast-extractor, call-graph, data-flow, compile-context, and run-all; replace these repetitive targets with a single pattern rule or macro to DRY the file: define a variable PHASE_BINARIES := scope-detector static-analysis ast-extractor call-graph data-flow compile-context run-all and add the pattern rule $(PHASE_BINARIES): with the shared recipe (@echo "Building $@..."; @mkdir -p $(BIN_DIR); @go build -o $(BIN_DIR)/$@ ./cmd/$@) and keep build-context as compile-context run-all to preserve the convenience target.



============================================================================
File: scripts/codereview/cmd/run-all/main.go
Line: 184 to 213
Type: potential_issue

Comment:
Path traversal check could be bypassed.

The strings.Contains(binDir, "..") check is insufficient. After filepath.Abs(), the path is already resolved, making the prior check redundant. A more robust approach would verify the resolved path is within expected bounds.



üîß Suggested improvement

 func validateBinDir(binDir string) error {
-       // Prevent path traversal sequences
-       if strings.Contains(binDir, "..") {
-               return fmt.Errorf("bin-dir cannot contain path traversal sequences '..'")
-       }
-
        // Get absolute path
        absPath, err := filepath.Abs(binDir)
        if err != nil {
                return fmt.Errorf("invalid bin-dir path: %w", err)
        }
+
+       // Clean the path to resolve any traversal
+       absPath = filepath.Clean(absPath)

        // Verify directory exists
        info, err := os.Stat(absPath)


The filepath.Abs() already resolves .. sequences, so the string check is redundant. The cleaned absolute path is what matters for validation.

Prompt for AI Agent:
In @scripts/codereview/cmd/run-all/main.go around lines 184 - 213, The current pre-Abs strings.Contains("..") check in validateBinDir is ineffective and redundant; instead remove that check, resolve the path with filepath.Abs (and filepath.Clean), then ensure the cleaned absolute path is constrained to an allowed base (e.g., application root or configured allowed directory) by using filepath.Rel(baseDir, absPath) and rejecting the path if the relative path begins with ".." (or equals "..") indicating traversal outside the base; keep the existing os.Stat and info.IsDir checks and return the same error messages on non-existence or non-directory.



============================================================================
File: scripts/codereview/install.sh
Line: 97 to 125
Type: potential_issue

Comment:
Update pinned Go tool versions‚Äîall are significantly outdated.

The pinned versions are several months to major versions behind the latest stable releases:
- staticcheck@v0.5.1 ‚Üí latest: 2025.1.1 (Mar 5, 2025)
- gosec@v2.21.4 ‚Üí latest: v2.22.11 (Dec 11, 2025)
- golangci-lint@v1.62.2 ‚Üí latest: v2.8.0 (Jan 7, 2026)

While no CVEs were found in the pinned versions, updating to the latest stable releases is essential for bug fixes, performance improvements, and staying current with security best practices.

Prompt for AI Agent:
In @scripts/codereview/install.sh around lines 97 - 125, The install_go_tools function pins outdated versions for staticcheck, gosec, and golangci-lint; update the pinned go install versions to current stable releases by replacing staticcheck@v0.5.1, gosec@v2.21.4, and golangci-lint@v1.62.2 with the latest tags (e.g., staticcheck@2025.1.1, gosec@v2.22.11, golangci-lint@v2.8.0) in the calls inside install_go_tools, keep the existing install_tool and verify_checksum calls/arguments unchanged, and run a quick CI pipeline or local install to verify the new binaries are found by command -v (staticcheck, gosec, golangci-lint) and that their --version output still parses correctly in the log_info lines.



============================================================================
File: scripts/codereview/cmd/run-all/main.go
Line: 389 to 422
Type: potential_issue

Comment:
Potential race: cmd.Process may be nil when Kill() is called.

There's a small race window between cmd.Run() starting and cmd.Process being set. If timeout/cancellation fires very quickly, cmd.Process could still be nil despite the nil check passing.



üîß Suggested fix using exec.CommandContext

-       // Create a channel for command completion
-       done := make(chan error, 1)
-       go func() {
-               done

Prompt for AI Agent:
In @scripts/codereview/cmd/run-all/main.go around lines 389 - 422, The code has a race on cmd.Process when Kill() is called; construct the command with exec.CommandContext(ctx, ...) (use the same args you pass to exec.Command) so the context drives subprocess termination instead of manually killing; then remove the manual cmd.Process.Kill() calls in the timeout and ctx.Done() branches (or at minimum stop checking cmd.Process and calling Kill) and rely on cmd.Run()/CommandContext to handle cleanup and exit codes; update references to cmd creation (where exec.Command(...) is used) to exec.CommandContext to fix the race.
